# DQN
Deep Q Neuralnetの略らしい  
困ったときの[Qiita](https://qiita.com/icoxfog417/items/242439ecd1a477ece312)を読んだ  
上記の記事によると、

> * States: S
> * 状況。ゲームでいえば、特定の局面を表す
> * Model: T(s, a, s') (=P(s'|s, a))
>   * TはTransitionで、状況sの時に行動aをとると状況s'になる、ということ。ただし、aを選択しても発動しなかったり別のところに行ったり、といった状況を表現するため確率的な表現(P(s'|s, a))になる
> * Actions: A(s), A
>   * 行動。状況によって取れる行動が変わる場合は、A(s)といった関数になる
> * Reward: R(s), R(s, a), R(s, a , s')
>   * 状況、またその状況における行動から得られる報酬。この報酬は、最後の結果以外は自己評価になります(即時報酬)。
> * Policy: $\pi(s)$ -> a
>   * 戦略。状況sにおいてどういう行動aを取るべきか、を返す関数。

このモデル化をMarkov Decision Process(MDP、マルコフ決定過程)といいます。マルコフというのはマルコフ性という意味で、$\pi(s)$で表されるように次の行動には現在の状態(s)しか関与しないという性質を表します。

らしい。

- sとして考えられるもの
  - 16個のマスの2の指数
  - ~~4方向の動作がそれぞれ可能か~~
- T(s, a, s) (=P(s'|s, a))
  - sは上で考えているのでよし、aは4通り以内
  - s'として考えられるのは得られる点数?
- a(s)
  - 4方向あるが、選択肢は4つ以下
- R(s)
  - 4方向に動いたときに追加で得られる点数
